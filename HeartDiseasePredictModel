# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/gist/theboredasian/a105dec29f5466d28809559d18e28062/untitled1.ipynb
"""

#description: to classify a person as having a cardiovasuclar disease or not

#import libraries
import numpy as np
import pandas as pd
import seaborn as sns

#Load the data
from google.colab import files
uploaded = files.upload()

#store the data into a variable
df = pd.read_csv('cardio_train.csv',sep=';')

#print the frist 7 rows of data
df.head(7)

df.shape

#count empty or no values in each column
df.isna().sum()

#another way to check for missing values
df.isnull().values.any()

#View some basic statistics
df.describe()

#get a count of number of patients with heart disease and without
df['cardio'].value_counts()

#Visualize the count
sns.countplot(df['cardio'])

#look at the number of people with cardiovascular disease that exceed the number of people withour cardiovascular disease

#create a years column
df['years'] = (df['age'] / 365).round(0)
df['years'] = pd.to_numeric(df['years'],downcast='integer')

#visualize
sns.countplot(x='years', hue='cardio', data=df, palette='colorblind', edgecolor=sns.color_palette('dark', n_colors=1))

#get the correlation of the columns
df.corr()

#visualize the data
import matplotlib.pyplot as plt
plt.figure(figsize=(7,7))
sns.heatmap(df.corr(),annot=True, fmt='.0%')

#remove the years column
df = df.drop('years', axis=1)

#remove the id
df = df.drop('id', axis=1)

#split the data into feature data and target data
X = df.iloc[:, :-1].values
Y = df.iloc[:, -1].values

#split the data again, into 75% training data set and 25 %testing data set
from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split(X , Y, test_size=0.25, random_state = 1)

#feature scaling
#scale the values in the data to be values between 0 and 1 iunclusive
from sklearn.preprocessing import StandardScaler
sc=StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

#use random forest classifier
from sklearn.ensemble import RandomForestClassifier
forest = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state=1)
forest.fit(X_train, Y_train)

#test the models accurary on the training set
model=forest
model.score(X_train, Y_train)

#test the models accurary on the test data
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(Y_test, model.predict(X_test))

TN= cm[0][0]
TP = cm[1][1]
FN = cm[1][0]
FP = cm[0][1]

print(cm)

#print the model accuracy
print('Model Test Accuracy = {}'.format( (TP+TN)  /(TP+TN+FN+FP) ))

new_input[11]=input()
new_output=model.predict(new_input)

print(new_input,new_output)
